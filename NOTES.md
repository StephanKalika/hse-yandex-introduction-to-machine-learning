# Решающие деревья

Решающие деревья относятся к классу логических методов. Их основная идея состоит в объединении определенного количества простых решающих правил, благодаря чему итоговый алгоритм является интерпретируемым. Как следует из названия, решающее дерево представляет собой бинарное дерево, в котором каждой вершине сопоставлено некоторое правило вида "j-й признак имеет значение меньше b". В листьях этого дерева записаны числа-предсказания. Чтобы получить ответ, нужно стартовать из корня и делать переходы либо в левое, либо в правое поддерево в зависимости от того, выполняется правило из текущей вершины или нет.

Одна из особенностей решающих деревьев заключается в том, что они позволяют получать важности всех используемых признаков. Важность признака можно оценить на основе того, как сильно улучшился критерий качества благодаря использованию этого признака в вершинах дерева.

#### Материалы

- [Подробнее про решающие деревья в sklearn](http://scikit-learn.org/stable/modules/tree.html)
- [Работа с пропущенными значениями в pandas](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)
- [Подробнее о деревьях и их построении](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture07-trees.pdf)

# Метрические методы
Метрические методы основаны на гипотезе компактности, суть которой состоит в том, что объекты с похожими признаковыми описаниями имеют похожие значения целевой переменной. Если эта гипотеза верна, то строить прогноз для нового объекта можно на основе близких к нему объектов из обучающей выборки — например, путем усреднения их ответов (для регрессии) или путем выбора наиболее популярного среди них класса (для классификации). Методы такого типа и называются метрическими. Они имеют несколько особенностей:

- Процедура обучения, по сути, отсутствует — достаточно лишь запомнить все объекты обучающей выборки
- Можно использовать метрику, учитывающую особенности конкретного набора данных — например, наличие категориальных (номинальных) признаков
- При правильном выборе метрики и достаточном размере обучающей выборки метрические алгоритмы показывают качество, близкое к оптимальному

Метрические методы чувствительны к масштабу признаков — так, если масштаб одного из признаков существенно превосходит масштабы остальных признаков, то их значения практически не будут влиять на ответы алгоритма. Поэтому важно производить масштабирование признаков. Обычно это делается путем вычитания среднего значения признака и деления на стандартное отклонение.

# Градиентные методы численной минимизации и алгоритм SG
Мы можем применять различные методы численной оптимизации для того, чтобы минимизировать наш аппроксимированный функционал, поскольку он теперь является непрерывной функцией или даже гладкой, в зависимости от того, какую функцию потерь мы использовали.
Самый, наверное, простой метод численной оптимизации — это метод **градиентного спуска**.
Он заключается в том, что сначала фиксируется некоторое начальное приближение для искомого вектора весов. Например, случайное. И затем делается последовательность градиентных шагов. То есть каждая итерация — это небольшое смещение вектора весов по антиградиенту.
Почему антиградиент? Градиент — это вектор, который показывает направление наискорейшего возрастания функции. Ну и, соответственно, минус этот вектор, или антиградиент, он показывает направление наискорейшего убывания.
То есть нам туда и надо идти, чтобы найти минимум функционала. Здесь возникает несколько проблем: проблема, будет ли этот метод сходиться; проблема выбора градиентного шага; проблема выбора начального приближения.
Так вот, идея сходимости, идея ускорения сходимости здесь заключается в том, чтобы не вычислять сумму сразу по всем объектам, а брать каждый объект... Ну обычно их берут в случайном порядке по одному. И после каждого объекта обновлять вектор весов.
Оказывается, что это приводит к существенному ускорению сходимости, это называется процедурой Роббинса–Монро и называется методом стохастической аппроксимации.
Процедура заключается в следующем: сначала инициализируется вектор весов, как это — мы чуть позже рассмотрим. Потом, при текущем положении вектора весов вычисляется оценка функционала качества на обучающей выборке, естественно, с нашей аппроксимированной функцией потерь, и затем начинается основной процесс.
На каждом шаге этого итерационного процесса мы выбираем объект и обучающие выборки случайным образом. Вычисляем значение функции потерь, обозначенного εi, и делаем градиентный шаг. А дальше, может быть, несколько нетривиальный ход, мы должны с учетом сделанной поправки к вектору весов переоценить значение функционала. Это нужно нам для того, чтобы понять, в какой момент нам останавливаться, когда значение функционала к чему-то сойдется или перестанет существенно меняться. Здесь использована формула экспоненциального скользящего среднего для того, чтобы подсчитать, ну пусть не точно значение функционала на всех объектах обучающей выборки, но по быстрой рекурентной формуле оценить среднее значение функционала, среднее значение функции потерь, которая получалась на последних итерациях.
Это тоже такой прием, который позволяет здесь не потерять вычислительную эффективность метода за счет вычисления функционала качества на всей обучающей выборке. 
Он часто используется при оптимизации, при конструировании критериев остановки. Итак, проблема. Нам после каждого шага по вектору w, который делается по одному объекту, поэтому делается он очень эффективно, хотелось бы оценить, насколько улучшилось качество классификации всей обучающей выборки по полученному обновленному вектору w. Конечно же, если мы начнем вычислять среднее по всей обучающей выборке, мы потеряем эффективность этого метода. Поэтому вот используется такая рекурентная формула. Она называется экспоненциальным скользящим средним, и проще всего ее объяснить, если привести ее аналогию с рекурентной формулой для вычисления среднего арифметического. Она приведена на слайде. Это очень простая формула, которая получается перегруппировкой слагаемых. Так вот, если в этой формуле 1 / m заменить на некоторую константу λ, то мы получим другой способ усреднения. И если расписать по полученной рекурентной формуле, что же за сумму мы вычисляем, то окажется, что это сумма всех значений εi-тых, но вес этих значений убывает по мере того, как это значение дальше остается в прошлом. То есть можно сказать, что это **темп забывания** тех ошибок, которые мы допускали на каждой итерации. Ну и можно оценить, что значение параметра λ, если мы его выберем, скажем, 1 поделить на 100, то это будет примерно эквивалентно тому, что мы усредняем значение функционала на ста последних объектов, которые мы брали для обучения в методе стохастического градиента.

# Линейные алгоритмы
Линейные алгоритмы — распространенный класс моделей, которые отличается своей простотой и скоростью работы. Их можно обучать за разумное время на очень больших объемах данных, и при этом они могут работать с любыми типами признаков — вещественными, категориальными, разреженными. В этом задании мы предлагаем вам воспользоваться персептроном — одним из простейших вариантов линейных моделей.

Как и в случае с метрическими методами, качество линейных алгоритмов зависит от некоторых свойств данных. В частности, признаки должны быть нормализованы, то есть иметь одинаковый масштаб. Если это не так, и масштаб одного признака сильно превосходит масштаб других, то качество может резко упасть.

Один из способов нормализации заключается в стандартизации признаков. Для этого берется набор значений признака на всех объектах, вычисляется их среднее значение и стандартное отклонение. После этого из всех значений признака вычитается среднее, и затем полученная разность делится на стандартное отклонение.

# Линейные методы
Линейные методы имеют несколько очень важных подвидов. Метод опорных векторов максимизирует отступы объектов, что тесно связано с минимизацией вероятности переобучения. При этом он позволяет очень легко перейти к построению нелинейной разделяющей поверхности благодаря ядровому переходу. Логистическая регрессия позволяет оценивать вероятности принадлежености классам, что оказывается полезным во многих прикладных задачах.

# Минимизация эмпирического риска
[Эмпирический риск](http://www.machinelearning.ru/wiki/index.php?title=%D0%AD%D0%BC%D0%BF%D0%B8%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D1%80%D0%B8%D1%81%D0%BA) (Empirical Risk) — это средняя величина ошибки алгоритма на обучающей выборке.


Метод минимизации эмпирического риска (Empirical Risk Minimization, ERM) — это общий подход к решению широкого класса задач обучения по прецедентам, в первую очередь — задач обучения с учителем, включая задачи классификации и регрессии.

#### Задача обучения по прецедентам
Пусть X — множество описаний объектов,  Y — множество допустимых ответов. Предполагается, что существует неизвестная целевая зависимость — отображение y^*: X -> Y, значения которой известны только на объектах конечной обучающей выборки X^m = {(x_1, y_1),...,(x_m,y_m)}.
Задача обучения по прецедентам состоит в том, чтобы построить алгоритм a: X -> Y, который приближал бы неизвестную целевую зависимость как на элементах выборки, так и на всём множестве X.

# Линейный классификатор
[Линейный классификатор](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80) — алгоритм классификации, основанный на построении линейной разделяющей поверхности. В случае двух классов разделяющей поверхностью является гиперплоскость, которая делит пространство признаков на два полупространства. В случае большего числа классов разделяющая поверхность кусочно-линейна.

# Машина опорных векторов
[Машина опорных векторов](http://www.machinelearning.ru/wiki/index.php?title=SVM) — является одной из наиболее популярных методологий обучения по прецедентам, предложенной В. Н. Вапником и известной в англоязычной литературе под названием SVM (Support Vector Machine).


Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin). Случай линейной разделимости. Задача квадратичного программирования. Опорные векторы. Случай отсутствия линейной разделимости. Функции ядра (kernel functions), спрямляющее пространство, теорема Мерсера. Способы построения ядер. Примеры ядер. Сопоставление SVM и нейронной RBF-сети. Обучение SVM методом активных ограничений. SVM-регрессия.


Метод опорных векторов (Support Vector Machine, SVM) — один из видов линейных классификаторов. Функционал, который он оптимизирует, направлен на максимизацию ширины разделяющей полосы между классами. Из теории статистического обучения известно, что эта ширина тесно связана с обобщающей способностью алгоритма, а ее максимизация позволяет бороться с переобучением.


Метод опорных векторов имеет еще одну особенность. Если преобразовать его оптимизационную задачу, то окажется, что итоговый классификатор можно представить как взвешенную сумму скалярных произведений данного объекта на объекты обучающей выборки&

По сути, алгоритм делает предсказания на основе сходства нового объекта с объектами обучающей выборки. При этом, как правило, далеко не все коэффициенты оказываются ненулевыми. Это означает, что классификация делается на основе сходства лишь с частью обучающих объектов. Такие объекты называются **опорными**.

# Логистическая регрессия
[Логистическая регрессия](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F) является одним из статистических методов классификации с использованием линейного дискриминанта Фишера.


В отличие от обычной регрессии, в методе логистической регрессии не производится предсказание значения числовой переменной исходя из выборки исходных значений. Вместо этого, значением функции является вероятность того, что данное исходное значение принадлежит к определенному классу. Для простоты, давайте предположим, что у нас есть только два класса и вероятность, которую мы будем определять, вероятности того, что некоторое значение принадлежит классу "+". И конечно P_ = 1 - P+. Таким образом, результат логистической регрессии всегда находится в интервале [0, 1].

Основная идея логистической регрессии заключается в том, что пространство исходных значений может быть разделено линейной границей (т.е. прямой) на две соответствующих классам области. Итак, что же имеется ввиду под линейной границей? В случае двух измерений — это просто прямая линия без изгибов. В случае трех — плоскость, и так далее. Эта граница задается в зависимости от имеющихся исходных данных и обучающего алгоритма. Чтобы все работало, точки исходных данных должны разделяться линейной границей на две вышеупомянутых области. Если точки исходных данных удовлетворяют этому требованию, то их можно назвать линейно разделяемыми.

1. Логистическая регрессия — одно из статистических методов классификации с использованием линейного дискриминанта Фишера.
2. Значением функции является вероятность того, что данное исходное значение принадлежит к определенному классу.
3. Механизм обучения логистической регрессии старается максимизировать среднее значение.
